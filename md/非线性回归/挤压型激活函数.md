<!--
 * @Author: matiastang
 * @Date: 2022-08-03 14:57:22
 * @LastEditors: matiastang
 * @LastEditTime: 2022-08-03 15:13:51
 * @FilePath: /matias-AI/md/非线性回归/挤压型激活函数.md
 * @Description: 挤压型激活函数
-->
# 挤压型激活函数

这一类函数的特点是，当输入值域的绝对值较大的时候，其输出在两端是饱和的，都具有`S形的函数曲线`以及`压缩输入值域`的作用，所以叫`挤压型激活函数`，又可以叫`饱和型激活函数`。

在英文中，通常用`Sigmoid`来表示，原意是`S`型的曲线，在数学中是指一类具有压缩作用的`S`型的函数，在神经网络中，有两个常用的`Sigmoid`函数，一个是`Logistic`函数，另一个是`Tanh`函数。下面我们分别来讲解它们。

## `Logistic`函数

`对数几率函数`（`Logistic Function`，简称对率函数）。

很多文字材料中通常把激活函数和分类函数混淆在一起说，有一个原因是：在二分类任务中最后一层使用的对率函数与在神经网络层与层之间连接的`Sigmoid`激活函数，是同样的形式。所以它既是激活函数，又是分类函数，是个特例。

Sigmoid(z) = 1 / (1 + e-z)

###  值域

* 输入值域：(-无穷，+无穷)
* 输出值域：（0，1）
* 导数值域：（0, 0.25]

### 优点

* 从函数图像来看，Sigmoid函数的作用是将输入压缩到(0，1)这个区间范围内，这种输出在0~1之间的函数可以用来模拟一些概率分布的情况。它还是一个连续函数，导数简单易求。
* 从数学上来看，Sigmoid函数对中央区的信号增益较大，对两侧区的信号增益小，在信号的特征空间映射上，有很好的效果。
* 从神经科学上来看，中央区酷似神经元的兴奋态，两侧区酷似神经元的抑制态，因而在神经网络学习方面，可以将重点特征推向中央区， 将非重点特征推向两侧区。

### 缺点

* 指数计算代价大。
* **反向传播时`梯度消失`**：从梯度图像中可以看到，Sigmoid的梯度在两端都会接近于0，根据链式法则，如果传回的误差是，那么梯度传递函数是，而这时接近零，也就是说整体的梯度也接近零。这就出现梯度消失的问题，并且这个问题可能导致网络收敛速度比较慢。
* 如果输入数据是`(-1, 1)`范围内的均匀分布的数据会导致什么样的结果呢？经过`Sigmoid`函数处理之后这些数据的均值就从`0`变到了`0.5`，导致了`均值`的`漂移`，在很多应用中，这个性质是不好的。

## `Tanh`函数

TanHyperbolic，即双曲正切函数。

###  值域

* 输入值域：(-无穷，+无穷)
* 输出值域：（-1，1）
* 导数值域：（0, 1）

### 优点

* 具有Sigmoid的所有优点。
* 无论从理论公式还是函数图像，这个函数都是一个和Sigmoid非常相像的激活函数，他们的性质也确实如此。但是比起Sigmoid，Tanh减少了一个缺点，就是他本身是零均值的，也就是说，在传递过程中，输入数据的均值并不会发生改变，这就使他在很多应用中能表现出比Sigmoid优异一些的效果。

### 缺点

* exp指数计算代价大。`梯度消失`问题仍然存在。

