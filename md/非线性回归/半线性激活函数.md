<!--
 * @Author: matiastang
 * @Date: 2022-08-03 15:15:33
 * @LastEditors: matiastang
 * @LastEditTime: 2022-08-03 15:21:11
 * @FilePath: /matias-AI/md/非线性回归/半线性激活函数.md
 * @Description: 半线性激活函数
-->
# 半线性激活函数

`半线性激活函数`又可以叫`非饱和型激活函数`。

## ReLU函数

Rectified Linear Unit，修正线性单元，线性整流函数，斜坡函数。

### 优点

反向导数恒等于1，更加有效率的反向传播梯度值，收敛速度快；
避免梯度消失问题；
计算简单，速度快；
活跃度的分散性使得神经网络的整体计算成本下降。

### 缺点⚓
无界。

梯度很大的时候可能导致的神经元“死”掉。

这个死掉的原因是什么呢？是因为很大的梯度导致更新之后的网络传递过来的输入是小于零的，从而导致ReLU的输出是0，计算所得的梯度是零，然后对应的神经元不更新，从而使ReLU输出恒为零，对应的神经元恒定不更新，等于这个ReLU失去了作为一个激活函数的作用。问题的关键点就在于输入小于零时，ReLU回传的梯度是零，从而导致了后面的不更新。在学习率设置不恰当的情况下，很有可能网络中大部分神经元“死”掉，也就是说不起作用了。

## LeakyReLU

### 优点
继承了ReLU函数的优点。

Leaky ReLU同样有收敛快速和运算复杂度低的优点，而且由于给了时一个比较小的梯度,使得时依旧可以进行梯度传递和更新，可以在一定程度上避免神经元“死”掉的问题。

## Softplus函数

## ELU函数