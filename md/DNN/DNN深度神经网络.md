<!--
 * @Author: matiastang
 * @Date: 2021-12-15 11:39:43
 * @LastEditors: matiastang
 * @LastEditTime: 2022-08-08 15:34:35
 * @FilePath: /matias-AI/md/DNN深度神经网络.md
 * @Description: DNN深度神经网络
-->
# DNN

DNN深度神经网络

全连接的`DNN`存在两个问题

1. 参数数量的膨胀。
2. 无法对时间序列上的变化进行建模。

对于第一个问题：

全连接`DNN`的结构里下层神经元和所有上层神经元都能够形成连接，带来的潜在问题是参数数量的膨胀。假设输入的是一幅像素为1K*1K的图像，隐含层有1M个节点，光这一层就有10^12个权重需要训练，这不仅容易`过拟合`，而且极容易陷入`局部最优`。另外，图像中有固有的局部模式（比如轮廓、边界，人的眼睛、鼻子、嘴等）可以利用，显然应该将图像处理中的概念和神经网络技术相结合。于是就出现了`卷积神经网络CNN`。对于`CNN`来说，并不是所有上下层神经元都能直接相连，而是通过`“卷积核”`作为中介。同一个卷积核在所有图像内是共享的，图像通过卷积操作后仍然保留原先的位置关系。

对于第二个问题：

样本出现的`时间顺序`对于`自然语言处理`、`语音识别`、`手写体识别`等应用非常重要。为了适应这种需求，就出现了另一种神经网络结构——循环神经网络`RNN`。

在普通的`全连接网络DNN`或`CNN`中，每层神经元的信号只能向上一层传播，样本的处理在各个时刻独立，因此又被成为前向神经网络(`Feed-forward Neural Networks`)。而在`RNN`中，神经元的输出可以在下一个时间戳直接作用到自身，即第i层神经元在m时刻的输入，除了（i-1）层神经元在该时刻的输出外，还包括其自身在（m-1）时刻的输出！

随着网络的加深，训练变得越来越困难，时间越来越长，原因可能是：

* 参数多
* 数据量大
* 梯度消失
* 损失函数坡度平缓

为了解决上面这些问题，科学家们在深入研究网络表现的前提下，发现在下面这些方向上经过一些努力，可以给深度网络的训练带来或多或少的改善：

* 权重矩阵初始化
* 批量归一化
* 梯度下降优化算法
* 自适应学习率算法

## 权重矩阵初始化

ID 网络深度	初始化方法	激活函数	说明
1	单层	零初始化	无	可以
2	双层	零初始化	Sigmoid	错误，不能进行正确的反向传播
3	双层	随机初始化	Sigmoid	可以
4	多层	随机初始化	Sigmoid	激活值分布成凹形，不利于反向传播
5	多层	Xavier初始化	Tanh	正确
6	多层	Xavier初始化	ReLU	激活值分布偏向0，不利于反向传播
7	多层	MSRA初始化	ReLU	正确

## 梯度下降优化算法

### 随机梯度下降 SGD

* 随机梯度下降算法，在当前点计算梯度，根据学习率前进到下一点。到中点附近时，由于样本误差或者学习率问题，会发生来回徘徊的现象，很可能会错过最优解。
* SGD的另外一个缺点就是收敛速度慢

### 动量算法 Momentum

SGD方法的一个缺点是其更新方向完全依赖于当前batch计算出的梯度，因而十分不稳定，因为数据有噪音。

Momentum算法借用了物理中的动量概念，它模拟的是物体运动时的惯性，即更新的时候在一定程度上保留之前更新的方向，同时利用当前batch的梯度微调最终的更新方向。这样一来，可以在一定程度上增加稳定性，从而学习地更快，并且还有一定摆脱局部最优的能力。Momentum算法会观察历史梯度，若当前梯度的方向与历史梯度一致（表明当前样本不太可能为异常点），则会增强这个方向的梯度。若当前梯度与历史梯度方向不一致，则梯度会衰减。

### 自适应学习率算法

#### AdaGrad
Adaptive subgradient method.

AdaGrad是一个基于梯度的优化算法，它的主要功能是：它对不同的参数调整学习率，具体而言，对低频出现的参数进行大的更新，对高频出现的参数进行小的更新。因此，他很适合于处理稀疏数据。

在这之前，我们对于所有的参数使用相同的学习率进行更新。但 Adagrad 则不然，对不同的训练迭代次数t，AdaGrad 对每个参数都有一个不同的学习率。这里开方、除法和乘法的运算都是按元素运算的。这些按元素运算使得目标函数自变量中每个元素都分别拥有自己的学习率。

#### AdaDelta
Adaptive Learning Rate Method. 

AdaDelta法是AdaGrad 法的一个延伸，它旨在解决它学习率不断单调下降的问题。相比计算之前所有梯度值的平方和，AdaDelta法仅计算在一个大小为w的时间区间内梯度值的累积和。

但该方法并不会存储之前梯度的平方值，而是将梯度值累积值按如下的方式递归地定义：关于过去梯度值的衰减均值，当前时间的梯度均值是基于过去梯度均值和当前梯度值平方的加权平均，其中是类似上述动量项的权值。

#### 均方根反向传播 RMSProp

Root Mean Square Prop。

RMSprop 是由 Geoff Hinton 在他 Coursera 课程中提出的一种适应性学习率方法，至今仍未被公开发表。RMSprop法要解决AdaGrad的学习率缩减问题。

#### Adam - Adaptive Moment Estimation

计算每个参数的自适应学习率，相当于RMSProp + Momentum的效果，Adam
算法在RMSProp算法基础上对小批量随机梯度也做了指数加权移动平均。和AdaGrad算法、RMSProp算法以及AdaDelta算法一样，目标函数自变量中每个元素都分别拥有自己的学习率。


### 批量归一化

有的书翻译成归一化，有的翻译成正则化，英文Batch Normalization，简称为BatchNorm，或BN。

既然可以把原始训练样本做归一化，那么如果在深度神经网络的每一层，都可以有类似的手段，也就是说把层之间传递的数据移到0点附近，那么训练效果就应该会很理想。这就是批归一化BN的想法的来源。

深度神经网络随着网络深度加深，训练起来越困难，收敛越来越慢，这是个在DL领域很接近本质的问题。很多论文都是解决这个问题的，比如ReLU激活函数，再比如Residual Network。BN本质上也是解释并从某个不同的角度来解决这个问题的。

BN就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同的分布，致力于将每一层的输入数据正则化成的分布。因次，每次训练的数据必须是mini-batch形式，一般取32，64等数值。
