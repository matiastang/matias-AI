<!--
 * @Author: matiastang
 * @Date: 2021-12-15 11:39:43
 * @LastEditors: matiastang
 * @LastEditTime: 2022-08-03 11:23:51
 * @FilePath: /matias-AI/md/DNN深度神经网络.md
 * @Description: DNN深度神经网络
-->
# DNN

DNN深度神经网络

全连接的`DNN`存在两个问题

1. 参数数量的膨胀。
2. 无法对时间序列上的变化进行建模。

对于第一个问题：

全连接`DNN`的结构里下层神经元和所有上层神经元都能够形成连接，带来的潜在问题是参数数量的膨胀。假设输入的是一幅像素为1K*1K的图像，隐含层有1M个节点，光这一层就有10^12个权重需要训练，这不仅容易`过拟合`，而且极容易陷入`局部最优`。另外，图像中有固有的局部模式（比如轮廓、边界，人的眼睛、鼻子、嘴等）可以利用，显然应该将图像处理中的概念和神经网络技术相结合。于是就出现了`卷积神经网络CNN`。对于`CNN`来说，并不是所有上下层神经元都能直接相连，而是通过`“卷积核”`作为中介。同一个卷积核在所有图像内是共享的，图像通过卷积操作后仍然保留原先的位置关系。

对于第二个问题：

样本出现的`时间顺序`对于`自然语言处理`、`语音识别`、`手写体识别`等应用非常重要。为了适应这种需求，就出现了另一种神经网络结构——循环神经网络`RNN`。

在普通的`全连接网络DNN`或`CNN`中，每层神经元的信号只能向上一层传播，样本的处理在各个时刻独立，因此又被成为前向神经网络(`Feed-forward Neural Networks`)。而在`RNN`中，神经元的输出可以在下一个时间戳直接作用到自身，即第i层神经元在m时刻的输入，除了（i-1）层神经元在该时刻的输出外，还包括其自身在（m-1）时刻的输出！