<!--
 * @Author: matiastang
 * @Date: 2021-12-15 11:42:46
 * @LastEditors: matiastang
 * @LastEditTime: 2022-08-03 14:25:11
 * @FilePath: /matias-AI/md/AI简介.md
 * @Description: AI简介
-->
[参考一](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652036408&idx=2&sn=c02e42f64b8319fcff8812ac5298c993&chksm=f121a9c9c65620dfacecc918e5bc4b8e9ef5af61886b514fde7216dfffaca83115ec0659fc68&scene=21#wechat_redirect)
[参考二](https://cloud.tencent.com/developer/article/1422815)

# 发展史

## 感知机

神经网络技术起源于上世纪五、六十年代，当时叫`感知机（perceptron）`，拥有`输入层`、`输出层`和`一个隐含层`。输入的特征向量通过隐含层变换达到输出层，在输出层得到分类结果。早期感知机的推动者是`Rosenblatt`。

## 多层感知机

随着数学的发展，这个缺点直到上世纪八十年代才被Rumelhart、Williams、Hinton、LeCun等人发明的`多层感知机（multilayer perceptron）`。

多层感知机可以摆脱早期离散传输函数的束缚，使用`sigmoid`或`tanh`等连续函数模拟神经元对激励的响应，在训练算法上则使用`Werbos`发明的`反向传播BP算法`。这就是我们现在所说的`神经网络NN`。

多层感知机解决了之前无法模拟`异或逻辑`的缺陷，同时更多的层数也让网络更能够刻画现实世界中的复杂情形。
多层感知机给我们带来的启示是：**神经网络的层数直接决定了它对现实的刻画能力**，利用每层更少的神经元拟合更加复杂的函数。

## DNN深度神经网络

全连接的`DNN`存在几个问题

1. 参数数量的膨胀。
2. 无法对时间序列上的变化进行建模。
3. 随着神经网络层数的加深，优化函数越来越容易陷入`局部最优解`，并且这个“陷阱”越来越偏离真正的全局最优。

对于第一个问题：

全连接`DNN`的结构里下层神经元和所有上层神经元都能够形成连接，带来的潜在问题是参数数量的膨胀。假设输入的是一幅像素为1K*1K的图像，隐含层有1M个节点，光这一层就有10^12个权重需要训练，这不仅容易`过拟合`，而且极容易陷入`局部最优`。另外，图像中有固有的局部模式（比如轮廓、边界，人的眼睛、鼻子、嘴等）可以利用，显然应该将图像处理中的概念和神经网络技术相结合。于是就出现了`卷积神经网络CNN`。对于`CNN`来说，并不是所有上下层神经元都能直接相连，而是通过`“卷积核”`作为中介。同一个卷积核在所有图像内是共享的，图像通过卷积操作后仍然保留原先的位置关系。

对于第二个问题：

样本出现的`时间顺序`对于`自然语言处理`、`语音识别`、`手写体识别`等应用非常重要。为了适应这种需求，就出现了另一种神经网络结构——循环神经网络`RNN`。

在普通的`全连接网络DNN`或`CNN`中，每层神经元的信号只能向上一层传播，样本的处理在各个时刻独立，因此又被成为前向神经网络(`Feed-forward Neural Networks`)。而在`RNN`中，神经元的输出可以在下一个时间戳直接作用到自身，即第i层神经元在m时刻的输入，除了（i-1）层神经元在该时刻的输出外，还包括其自身在（m-1）时刻的输出！

对于第三个问题：

2006年，`Hinton`利用预训练方法`缓解`了`局部最优解问题`。

为了克服`梯度消失`，`ReLU`、`maxout`等传输函数代替了`sigmoid`，形成了如今`DNN`的基本形式。**单从结构上来说，`全连接的DNN`和图1的`多层感知机`是没有任何区别的。**

`高速公路网络（highway network）`和`深度残差学习（deep residual learning）`进一步避免了`梯度消失`。

### 高速公路网络（highway network）

### 深度残差学习（deep residual learning）

## CNN卷积神经网络

## RNN循环神经网络

`（t+1）`时刻网络的最终结果`O(t+1)`是该时刻输入和所有历史共同作用的结果！这就达到了对时间序列建模的目的。但`RNN`可以看成一个在时间上传递的神经网络，它的深度是时间的长度！`**“梯度消失”**现象又要出现了，只不过这次发生在时间轴上`。

为了解决时间上的`梯度消失`，机器学习领域发展出了`长短时记忆单元LSTM`。

## LSTM长短时记忆单元

**通过门的开关实现时间上记忆功能，并防止梯度消失**

## 双向RNN、双向LSTM

RNN既然能继承历史信息，是不是也能吸收点未来的信息呢？因为在序列信号分析中，如果我能预知未来，对识别一定也是有所帮助的。因此就有了`双向RNN`、`双向LSTM`，**同时利用历史和未来的信息**。

## 总结

事实上，不论是那种网络，他们在实际应用中常常都混合着使用，比如`CNN`和`RNN`在上层输出之前往往会接上全连接层，很难说某个网络到底属于哪个类别。